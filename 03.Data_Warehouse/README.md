# Building Data Warehouse

## Introduction

- Sparkify wants to analyze the data they've been collecting on songs and user activity on their new music streaming app.
- Understand what songs users are listening to.
- Job is to create a database schema and ETL pipeline for this analysis.
- Objective is to build an ETL pipeline that extracts data from S3, stages into Redshift, and transform data into set of dimensional and fact tables for analytics use cases.

### DATA

The first dataset is a subset of real data from the [Million Song Dataset](https://labrosa.ee.columbia.edu/millionsong/). Each file is in JSON format and contains metadata about a song and the artist of that song.

- Song Dataset
    - contains metadata about a song and the artist of that song.
    - sample record
        - `{"num_songs": 1, "artist_id": "ARJIE2Y1187B994AB7", "artist_latitude": null, "artist_longitude": null, "artist_location": "", "artist_name": "Line Renaud", "song_id": "SOUPIRU12A6D4FA1E1", "title": "Der Kleine Dompfaff", "duration": 152.92036, "year": 0}`

The second dataset consists of log files in JSON format generated by an [event simulator](https://github.com/Interana/eventsim) based on the songs in the dataset above.

- Log Dataset
    - activity logs from a music streaming app.
    - sample log
        - `{"artist": null, "auth": "Logged In", "firstName": "Walter", "gender": "M", "itemInSession": 0, "lastName": "Frye", "length": null, "level": "free", "location": "San Francisco-Oakland-Hayward, CA", "method": "GET","page": "Home", "registration": 1540919166796.0, "sessionId": 38, "song": null, "status": 200, "ts": 1541105830796, "userAgent": "\"Mozilla\/5.0 (Macintosh; Intel Mac OS X 10_9_4) AppleWebKit\/537.36 (KHTML, like Gecko) Chrome\/36.0.1985.143 Safari\/537.36\"", "userId": "39"}`

### Schema for Song Play Analysis

- Fact Table
    - songplays
        - `songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent`
- Dimension Tables
    - users
        - `user_id, first_name, last_name, gender, level`
    - songs
        - `song_id, title, artist_id, year, duration`
    - artists
        - `artist_id, name, location, latitude, longitude`
    - time
        - `start_time, hour, day, week, month, year, weekday`

The schema is defined as per STAR schema.

### Project Files

1. `create_tables.py` contains code for setting up database and creates fact and dimensions table.
2. `sql_queries.py` contains sql queries, dropping and creating fact and dimensions tables in addition to insertion query.
3. `redshift_cluster_test.ipynb` contains Infrastructure as Code snippet for creation of redhift cluster, creating IAM policy and insertion of data from S3 to staging tables and then to fact and dimension tables.
4. `etl.py` reads and processes files from `song_data` and `log_data` and loads them into tables.

### Instructions

> Step by Step instructions are followed in `redshift_cluster_test.ipynb` notebook (for the reference).

1. Import necessary libraries
2. Add configuration of AWS Cluster, Database, IAM_ROLE, S3 data path to configuration file.
3. Create objects for redshift, s3, iam using boto3.
4. Check log files and song data files in S3 bucket.
5. Create an IAM User Role, assign appropriate permissions and create the Redshift Cluster.
6. Get the Endpoint value and Role for put into main configuration file.
7. Authorize Security Access Group to Default TCP/IP Address.
8. Launch database connectivity configuration.
9. Run the command "python create_tables.py" and then "python etl.py"
10. Query database to check results.
11. Delete the cluster, roles and assigned permission (to save cost).

